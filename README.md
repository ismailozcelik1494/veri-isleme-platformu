# ğŸš€ Veri Ä°ÅŸleme ve Analiz Platformu

Bu proje, Docker konteynerlerinde Ã§alÄ±ÅŸan Apache Airflow, Apache Spark, Redis, PostgreSQL ve MinIO servislerini entegre eden kapsamlÄ± bir veri iÅŸleme altyapÄ±sÄ± sunar. Senior Software Engineer pozisyonu iÃ§in teknik mÃ¼lakat gereksinimlerini karÅŸÄ±lamak iÃ§in oluÅŸturulmuÅŸ olup, Ã¶zellikle yaÅŸ ve kan grubu kriterleri Ã¼zerinden veri analizi yapan ve bu verileri otomatik olarak iÅŸleyen bir sistem sunmaktadÄ±r.

## ğŸ“‹ Ã–zellikler

- **Apache Airflow** ile iÅŸ akÄ±ÅŸÄ± otomasyonu ve periyodik gÃ¶rev Ã§alÄ±ÅŸtÄ±rma (her 10 dakikada bir)
- **Apache Spark** ile paralel ve daÄŸÄ±tÄ±k veri iÅŸleme
- **PostgreSQL** veritabanÄ± entegrasyonu ve veri depolama
- **MinIO** nesne depolama servisi
- **Docker Compose** ile mikroservis mimarisi
- Otomatik veri iÅŸleme ve filtreleme (yaÅŸ, kan grubu)
- Veri doÄŸrulama ve hata yÃ¶netimi
- **Flask** ile RESTful API implementasyonu
- VeritabanÄ± performans optimizasyonu (indeksleme)
- PgAdmin ile veritabanÄ± yÃ¶netimi

## ğŸ”§ Sistem Gereksinimleri

- Docker ve Docker Compose
- Python 3.11+
- Java JDK (Spark iÃ§in gerekli)
- En az 8GB RAM (tÃ¼m servislerin saÄŸlÄ±klÄ± Ã§alÄ±ÅŸmasÄ± iÃ§in)
- 15-20GB disk alanÄ±

## ğŸ“ Proje YapÄ±sÄ±

```
ISMAILOZCELIK-DOCKER/
â”œâ”€â”€ _archive/                               # ArÅŸivlenmiÅŸ eski dosyalar
|   |__ dependencies
â”‚   |   â”œâ”€â”€ __pycache__/                   # Python iÃ§in gerekli kÃ¼tÃ¼phaneler
â”‚   |   â”œâ”€â”€ abseil-cpp-master/             # Google'Ä±n C++ utility kÃ¼tÃ¼phanesi
â”‚   |   â””â”€â”€ re2-main/                      # Google'Ä±n RE2 regex kÃ¼tÃ¼phanesi
|   |â”€â”€ docker-compose/                    # Docker Compose yapÄ±landÄ±rmalarÄ±
â”‚   |   â””â”€â”€ data/                          # Ã–rnek veri dosyalarÄ±
â”‚   |   |   â”œâ”€â”€ country_data.csv           # Ãœlke verileri
â”‚   |   |   â”œâ”€â”€ create_table_query.sql     # Tablo oluÅŸturma SQL sorgularÄ±
â”‚   |   |   â”œâ”€â”€ db_index_query.sql         # Indexleme SQL sorgularÄ±
â”‚   |   |   â”œâ”€â”€ person_data.csv            # KiÅŸi verileri
â”‚   |   |   â””â”€â”€ ...
|   |   â”œâ”€â”€ docker-compose.yaml
|   |   â”œâ”€â”€ pg_hba.conf                    # PostgreSQL eriÅŸim yapÄ±landÄ±rmasÄ±
|   |   â””â”€â”€ yeni-docker-compose.yaml
|   â””â”€â”€ old-setups/
â”‚       â””â”€â”€ jdk-9.0.4_windows-x64_bin.exe 
â”‚
â”œâ”€â”€ airflow/                                # Airflow ile ilgili dosyalar (mevcut konumunda)
â”‚
â”œâ”€â”€ app/                                    # Ek Paketler ve Python kodlarÄ± iÃ§erir.
â”‚   â”œâ”€â”€ hadoop/                             # Hadoop iÃ§in gerekli kÃ¼tÃ¼phane dosyalarÄ±
â”‚   â”œâ”€â”€ java/                               # Java iÃ§in gerekli kÃ¼tÃ¼phane dosyalarÄ±
â”‚   â””â”€â”€ python/                             # Python kodlarÄ±
â”‚       â”œâ”€â”€ airflow_create_user.py          # Airflow kullanÄ±cÄ± oluÅŸturma
â”‚       â”œâ”€â”€ data-bucket.py                  # .csv dosyalarÄ±nÄ± Minio'da aktarma iÃ§in
â”‚       â”œâ”€â”€ error_handling.py               # Hata yÃ¶netimi ve veri doÄŸrulama
â”‚       â”œâ”€â”€ fernet.py                       # Fernet anahtar yÃ¶netimi (gÃ¼venlik)
â”‚       â”œâ”€â”€ istemci.py                      # YardÄ±mcÄ± betik
â”‚       â”œâ”€â”€ my_airflow_dag.py               # Spark iÅŸini Ã§alÄ±ÅŸtÄ±ran Airflow DAG
â”‚       â”œâ”€â”€ rest_api.py                     # REST API modÃ¼lÃ¼
â”‚       â”œâ”€â”€ spark_job.py                    # Spark veri iÅŸleme betiÄŸi
â”‚       â”œâ”€â”€ sunucu.py                       # Sunucu kontrolÃ¼ saÄŸlayan betik
â”‚       â””â”€â”€ test_app.py                     # Spark login iÅŸlemi saÄŸlayan betik
â”‚
â”œâ”€â”€ backups/                                # Yedekleme
â”‚   â””â”€â”€ docker_volumes_backup/              # Docker yedeklemeleri
â”‚
â”œâ”€â”€ config/                                 # YapÄ±landÄ±rma dosyalarÄ±
â”‚
â”œâ”€â”€ dags/                                   # Airflow DAG'larÄ±
â”‚   â”œâ”€â”€ __pycache__/                        # Python Ã¶nbellek dosyalarÄ±
â”‚   |   â”œâ”€â”€ my_airflow_dag.cpython-312.pyc
â”‚   â””â”€â”€ my_airflow_dag.py                   # Spark iÅŸini Ã§alÄ±ÅŸtÄ±ran Airflow DAG
|
â”œâ”€â”€ logs/                        # Log dosyalarÄ±
â”‚   â”œâ”€â”€ dag_id=spark_processing/ # Spark iÅŸleme gÃ¶revleri ile ilgili loglar
â”‚   â”œâ”€â”€ dag_processor_manager/   # DAG processor yÃ¶neticisi loglarÄ±
â”‚   â””â”€â”€ scheduler/               # Scheduler loglarÄ±
â”‚
â”œâ”€â”€ plugins/                      # Airflow eklentileri
|
â”œâ”€â”€ .env                          # Ã‡evre deÄŸiÅŸkenleri
â”‚
â”œâ”€â”€ docker-compose.yaml           # Ana docker-compose dosyasÄ± (birleÅŸtirilmiÅŸ)
â”‚
â”œâ”€â”€ dockerfile.dockerfile         # Ã–zel Dockerfile
â”‚
â”œâ”€â”€ flask_app.py                  # Flask uygulamasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in gerekli kodlarÄ± iÃ§erir.
â”‚
â”œâ”€â”€ init-db.sql                   # PostgreSQL baÅŸlangÄ±Ã§ script'i
â”‚
â”œâ”€â”€ pgadmin-servers.json          # PgAdmin yapÄ±landÄ±rmasÄ±
â”‚
â”œâ”€â”€ README.md                     # DÃ¶kÃ¼mantasyon
â”‚
â”œâ”€â”€ setup.bat                     # BaÅŸlama BetiÄŸi(Windows iÃ§in)
â”‚
â””â”€â”€ setup.sh                      # BaÅŸlama BetiÄŸi(Mac/Linux iÃ§in)

```

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### 1. Ã–n Gereksinimler

Docker ve Docker Compose kurulumunuzu kontrol edin:
```bash
docker --version
docker-compose --version
```

### 2. Proje Dizini OluÅŸturma ve DosyalarÄ± HazÄ±rlama
```bash
mkdir ismailozcelik-docker
cd ismailozcelik-docker
```

### 3. Docker Compose ile Airflow Kurulumu

Airflow Docker Compose dosyasÄ±nÄ± indirin:
```bash
curl 'https://airflow.apache.org/docs/apache-airflow/2.10.5/docker-compose.yaml' -o 'docker-compose.yaml'
```

Gerekli dizinleri oluÅŸturun:
```bash
mkdir dags logs plugins
```

### 4. Airflow Servislerini BaÅŸlatma

```bash
# Airflow'u baÅŸlatÄ±n
docker compose up airflow-init
docker compose up -d
```

### 5. Yeni Docker Compose DosyasÄ± ile Ek Servisleri BaÅŸlatma

Yeni servisleri (PgAdmin, Spark Master, Spark Worker, MinIO) iÃ§eren yeni-docker-compose.yaml dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:
```bash
docker-compose -f yeni-docker-compose.yaml up -d
```

### 6. Servislere EriÅŸim Bilgileri

| Servis | URL | KullanÄ±cÄ± | Åifre |
|--------|-----|-----------|-------|
| PgAdmin | http://localhost:5050/browser/ | airflow | airflow |
| Airflow | http://localhost:8081/ | airflow | airflow |
| MinIO | http://localhost:9001/browser | minioadmin | minioadmin |
| Spark UI | http://localhost:8082/ | - | - |
| Flask API | http://127.0.0.1:5000/results/[country_id] | - | - |

### 7. PostgreSQL VeritabanÄ± Kurulumu

PgAdmin'e giriÅŸ yapÄ±n ve yeni bir server ekleyin:
- Name: localhost
- Host name/address: postgres
- Port: 5432
- Maintenance database: postgres
- Username: airflow
- Password: airflow

### 8. VeritabanÄ± TablolarÄ±nÄ± OluÅŸturma

PgAdmin'de aÅŸaÄŸÄ±daki SQL sorgularÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:

```sql
CREATE TABLE country_data (
    country VARCHAR(10) PRIMARY KEY,
    country_name VARCHAR(100) NOT NULL
);

CREATE TABLE person_data (
    id SERIAL PRIMARY KEY,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    birthday DATE,
    blood_type VARCHAR(10),
    country VARCHAR(10) REFERENCES country_data(country) ON DELETE SET NULL
);

CREATE TABLE temp_person_data (
    first_name VARCHAR(250),
    last_name VARCHAR(250),
    birthday TEXT,
    blood_type VARCHAR(100),
    country VARCHAR(250)
);
```

### 9. Veri YÃ¼kleme ve Temizleme

CSV verilerini PostgreSQL'e yÃ¼kledikten sonra, gereksiz verileri temizleyin:
```sql
-- BaÅŸlÄ±k satÄ±rlarÄ±nÄ± sil
DELETE FROM public.temp_person_data WHERE first_name = 'first_name';
DELETE FROM public.country_data WHERE country = 'country';

-- Bozuk tarih formatlarÄ±nÄ± kontrol et
SELECT birthday 
FROM temp_person_data 
WHERE NOT (birthday ~ '^[0-9]{2}-[0-9]{2}-[0-9]{4}$' 
           AND SUBSTRING(birthday, 4, 2)::integer BETWEEN 1 AND 12);

-- DÃ¼zeltilmiÅŸ verileri ana tabloya aktar
INSERT INTO person_data (first_name, last_name, birthday, blood_type, country)
SELECT first_name, last_name, TO_DATE(birthday, 'DD-MM-YYYY'), blood_type, country
FROM temp_person_data;
```

### 10. Spark Job'unu Konteynere Kopyalama ve Ã‡alÄ±ÅŸtÄ±rma

```bash
# PostgreSQL JDBC sÃ¼rÃ¼cÃ¼sÃ¼nÃ¼ kopyalama
docker cp ./postgresql-42.7.5.jar ismailozcelik-docker-spark-master-1:/tmp/

# Spark iÅŸleme dosyasÄ±nÄ± kopyalama
docker cp ./spark_job.py ismailozcelik-docker-spark-master-1:/tmp/

# Spark job'unu Ã§alÄ±ÅŸtÄ±rma
docker exec -it ismailozcelik-docker-spark-master-1 spark-submit --master spark://ismailozcelik-docker-spark-master-1:7077 --jars "/tmp/postgresql-42.7.5.jar" /tmp/spark_job.py
```

### 11. Airflow DAG DosyasÄ±nÄ± Konteynere Kopyalama

```bash
docker cp ./my_airflow_dag.py ismailozcelik-docker-airflow-webserver-1:/opt/airflow/dags/
```

### 12. Hata YÃ¶netimi DosyasÄ±nÄ± Konteynere Kopyalama

```bash
docker cp ./error_handling.py ismailozcelik-docker-spark-master-1:/tmp/
docker exec -it ismailozcelik-docker-spark-master-1 spark-submit --master spark://ismailozcelik-docker-spark-master-1:7077 /tmp/error_handling.py
```

### 13. VeritabanÄ± Optimizasyonu

PgAdmin'de aÅŸaÄŸÄ±daki indeksleme sorgularÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:
```sql
CREATE INDEX idx_birthday ON person_data(birthday);
CREATE INDEX idx_blood_type ON person_data(blood_type);
CREATE INDEX idx_country ON person_data(country);
```

### 14. Flask API'sini BaÅŸlatma

Gerekli Python kÃ¼tÃ¼phanelerini kurun:
```bash
pip install flask
pip install psycopg2-binary
```

Flask uygulamasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:
```bash
python flask_app.py
```

## ğŸ“Š Veri Ä°ÅŸleme DetaylarÄ±

### Spark Ä°ÅŸleme Ä°ÅŸ AkÄ±ÅŸÄ±

1. **Veri Okuma**: PostgreSQL'den `person_data` ve `country_data` tablolarÄ±ndaki veriler okunur.
2. **Filtreleme**: Veriler aÅŸaÄŸÄ±daki kriterlere gÃ¶re filtrelenir:
   - YaÅŸÄ± 30'dan bÃ¼yÃ¼k olan kiÅŸiler
   - Kan grubu 'A+', 'A-', 'AB+' veya 'AB-' olan kiÅŸiler
3. **Gruplama ve BirleÅŸtirme**: Her Ã¼lke iÃ§in bu kriterlere uyan kiÅŸilerin isimleri birleÅŸtirilir.
4. **SonuÃ§ Yazma**: Ä°ÅŸlenmiÅŸ veriler `processed_results` tablosuna kaydedilir.

### Airflow DAG AÃ§Ä±klamasÄ±

Airflow DAG'Ä±, Spark iÅŸini her 10 dakikada bir Ã§alÄ±ÅŸtÄ±racak ÅŸekilde yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r. DAG yapÄ±landÄ±rmasÄ± ÅŸu ÅŸekildedir:

```python
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG('spark_processing', default_args=default_args, schedule_interval='*/10 * * * *')

spark_job = BashOperator(
    task_id='run_spark_job',
    bash_command='spark-submit --master spark://d11336fbe563:7077 --conf "spark.ui.showConsoleProgress=true" --jars /tmp/postgresql-42.7.5.jar /tmp/spark_job.py',
    dag=dag,
)
```

## ğŸ› ï¸ Hata YÃ¶netimi ve Veri DoÄŸrulama

Hata yÃ¶netimi aÅŸaÄŸÄ±daki durumlarÄ± kontrol eder:

1. **Eksik Veri KontrolÃ¼**: NULL deÄŸerleri tespit edilir
2. **Format DoÄŸrulama**: Tarih formatÄ±, yaÅŸ deÄŸerleri gibi alanlarÄ±n doÄŸruluÄŸu kontrol edilir
3. **Bozuk Veri Tespiti**: YanlÄ±ÅŸ formatlar filtrelenir (Ã¶rn. geÃ§ersiz tarihler)
4. **Hata LoglamasÄ±**: Hatalar error_logs tablosuna kaydedilir

## ğŸŒ REST API AyrÄ±ntÄ±larÄ±

Flask API, processed_results tablosundaki verileri JSON formatÄ±nda sunar:

**Endpoint**:
```
GET /results/<country_id>
http://127.0.0.1:5000/results/9
```

**Ã–rnek YanÄ±t**:
```json
[
  [
    "10",
    "Birol, NurgÃ¼n, Ayliz"
  ]
]
```

## ğŸ§ª Yerel GeliÅŸtirme OrtamÄ±

### Hadoop ve Spark iÃ§in Yerel Ortam AyarlarÄ±

1. Spark indirin (spark-3.5.5-bin-hadoop3.tgz)
2. Hadoop dizinlerini oluÅŸturun:
```bash
mkdir C:\hadoop
mkdir C:\hadoop\bin
```

3. Ortam deÄŸiÅŸkenlerini ayarlayÄ±n:
```bash
# Windows Ortam DeÄŸiÅŸkenleri
HADOOP_HOME = C:\hadoop
PYSPARK_PYTHON = C:\Python311
SPARK_HOME = C:\spark
```

4. PATH deÄŸiÅŸkenine ekleyin:
```
C:\Program Files\Java\jdk-9.0.4\bin
C:\spark\bin
C:\Python311
C:\Python311\Scripts
```

5. Hadoop iÃ§in dizin izinlerini ayarlayÄ±n:
```powershell
mkdir C:\spark-events -Force
icacls "C:\hadoop" /grant "Everyone:(OI)(CI)F" /T
icacls "C:\temp" /grant "Everyone:(OI)(CI)F" /T
icacls "C:\spark-events" /grant "Everyone:(OI)(CI)F" /T
```

## ğŸ” Sorun Giderme

### Docker Konteynerlerini Listeleme
```bash
docker ps
```

### Docker Konteynerlerini Durdurma
```bash
docker-compose down
```

### Log DosyalarÄ±nÄ± Ä°nceleme
```bash
docker logs spark-master
```

### PostgreSQL YapÄ±landÄ±rmasÄ±nÄ± DeÄŸiÅŸtirme
```bash
docker cp postgres-container_analytcis:/var/lib/postgresql/data/pg_hba.conf ./pg_hba.conf
docker cp ./pg_hba.conf postgres-container_analytcis:/var/lib/postgresql/data/pg_hba.conf
```

### KÃ¼tÃ¼phane VersiyonlarÄ±nÄ± Kontrol Etme
```bash
pip show flask
pip show psycopg2
pip show psycopg2-binary
```

## ğŸ“ Ã–nemli Notlar

1. Docker servislerinin doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun
2. Airflow DAG'Ä±nÄ±n aktif olduÄŸundan emin olun
3. PostgreSQL JDBC sÃ¼rÃ¼cÃ¼sÃ¼nÃ¼n (`postgresql-42.7.5.jar`) doÄŸru yerde olduÄŸunu kontrol edin
4. Veri aktarÄ±mÄ±ndan Ã¶nce CSV dosyalarÄ±nÄ±n formatÄ±nÄ± doÄŸrulayÄ±n
5. API'yi test etmek iÃ§in farklÄ± country_id deÄŸerleri deneyin

---

## ğŸ“¦ Referanslar ve Kaynaklar

- [Apache Airflow DokÃ¼mantasyonu](https://airflow.apache.org/docs/)
- [Apache Spark DokÃ¼mantasyonu](https://spark.apache.org/docs/latest/)
- [PostgreSQL DokÃ¼mantasyonu](https://www.postgresql.org/docs/)
- [MinIO DokÃ¼mantasyonu](https://min.io/docs/minio/container/index.html)
- [Flask DokÃ¼mantasyonu](https://flask.palletsprojects.com/)
- [Docker DokÃ¼mantasyonu](https://docs.docker.com/)